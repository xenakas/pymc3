
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Using a “black box” likelihood function &#8212; PyMC3 3.5 documentation</title>
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="shortcut icon" href="../_static/PyMC3.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Bayesian Estimation Supersedes the T-Test" href="BEST.html" />
    <link rel="prev" title="Compound Steps in Sampling" href="sampling_compound_step.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #303F9F;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #D84315;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 8ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #cfcfcf;
    border-radius: 2px;
    background: #f7f7f7;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.pngmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }
</style>
<div class="section" id="Using-a-&quot;black-box&quot;-likelihood-function">
<h1>Using a “black box” likelihood function<a class="headerlink" href="#Using-a-"black-box"-likelihood-function" title="Permalink to this headline">¶</a></h1>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="o">%</span><span class="k">load_ext</span> Cython

<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="kn">import</span> <span class="nn">pymc3</span> <span class="kn">as</span> <span class="nn">pm</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;seaborn-darkgrid&#39;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Running on PyMC3 v{}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pm</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>

<span class="c1"># for reproducibility here&#39;s some version info for modules used in this notebook</span>
<span class="kn">import</span> <span class="nn">theano</span>
<span class="kn">import</span> <span class="nn">theano.tensor</span> <span class="kn">as</span> <span class="nn">tt</span>
<span class="kn">import</span> <span class="nn">platform</span>
<span class="kn">import</span> <span class="nn">cython</span>
<span class="kn">import</span> <span class="nn">IPython</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">emcee</span>
<span class="kn">import</span> <span class="nn">corner</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Python version:     {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">platform</span><span class="o">.</span><span class="n">python_version</span><span class="p">()))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;IPython version:    {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">IPython</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Cython version:     {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cython</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;GSL version:        {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">popen</span><span class="p">(</span><span class="s1">&#39;gsl-config --version&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">strip</span><span class="p">()))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Numpy version:      {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Theano version:     {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">theano</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Matplotlib version: {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">matplotlib</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;emcee version:      {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">emcee</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;corner version:     {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">corner</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Running on PyMC3 v3.5
Python version:     3.6.6
IPython version:    6.5.0
Cython version:     0.28.5
GSL version:        2.1
Numpy version:      1.15.1
Theano version:     1.0.2
Matplotlib version: 2.2.3
emcee version:      2.2.1
corner version:     2.0.1
</pre></div></div>
</div>
<p><a class="reference external" href="https://docs.pymc.io/index.html">PyMC3</a> is a great tool for doing
Bayesian inference and parameter estimation. It has a load of <a class="reference external" href="https://docs.pymc.io/api/distributions.html">in-built
probability
distributions</a> that you
can use to set up priors and likelihood functions for your particular
model. You can even create your own <a class="reference external" href="https://docs.pymc.io/prob_dists.html#custom-distributions">custom
distributions</a>.</p>
<p>However, this is not necessarily that simple if you have a model
function, or probability distribution, that, for example, relies on an
external code that you have little/no control over (and may even be, for
example, wrapped <code class="docutils literal notranslate"><span class="pre">C</span></code> code rather than Python). This can be problematic
went you need to pass parameters set as PyMC3 distributions to these
external functions; your external function probably wants you to pass it
floating point numbers rather than PyMC3 distributions!</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pymc3</span> <span class="kn">as</span> <span class="nn">pm</span><span class="p">:</span>
<span class="kn">from</span> <span class="nn">external_module</span> <span class="kn">import</span> <span class="n">my_external_func</span>  <span class="c1"># your external function!</span>

<span class="c1"># set up your model</span>
<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">():</span>
    <span class="c1"># your external function takes two parameters, a and b, with Uniform priors</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>

    <span class="n">m</span> <span class="o">=</span> <span class="n">my_external_func</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>  <span class="c1"># &lt;--- this is not going to work!</span>
</pre></div>
</div>
<p>Another issue is that if you want to be able to use the <a class="reference external" href="https://docs.pymc.io/notebooks/getting_started.html#Gradient-based-sampling-methods">gradient-based
step
samplers</a>
like
<a class="reference external" href="https://docs.pymc.io/api/inference.html#module-pymc3.step_methods.hmc.nuts">NUTS</a>
and <a class="reference external" href="https://docs.pymc.io/api/inference.html#hamiltonian-monte-carlo">Hamiltonian Monte Carlo
(HMC)</a>,
then your model/likelihood needs a gradient to be defined. If you have a
model that is defined as a set of Theano operators then this is no
problem - internally it will be able to do automatic differentiation -
but if your model is essentially a “black box” then you won’t
necessarily know what the gradients are.</p>
<p>Defining a model/likelihood that PyMC3 can use and that calls your
“black box” function is possible, but it relies on creating a <a class="reference external" href="https://docs.pymc.io/advanced_theano.html#writing-custom-theano-ops">custom
Theano
Op</a>.
This is, hopefully, a clear description of how to do this, including one
way of writing a gradient function that could be generally applicable.</p>
<p>In the examples below, we create a very simple model and log-likelihood
function in <a class="reference external" href="http://cython.org/">Cython</a>. Cython is used just as an
example to show what you might need to do if calling external <code class="docutils literal notranslate"><span class="pre">C</span></code>
codes, but you could in fact be using pure Python codes. The
log-likelihood function used is actually just a <a class="reference external" href="https://en.wikipedia.org/wiki/Normal_distribution">Normal
distribution</a>, so
defining this yourself is obviously overkill (and I’ll compare it to
doing the same thing purely with the pre-defined PyMC3
<a class="reference external" href="https://docs.pymc.io/api/distributions/continuous.html#pymc3.distributions.continuous.Normal">Normal</a>
distribution), but it should provide a simple to follow demonstration.</p>
<p>First, let’s define a <em>super-complicated</em>™ model (a straight line!),
which is parameterised by two variables (a gradient <code class="docutils literal notranslate"><span class="pre">m</span></code> and a
y-intercept <code class="docutils literal notranslate"><span class="pre">c</span></code>) and calculated at a vector of points <code class="docutils literal notranslate"><span class="pre">x</span></code>. Here the
model is defined in <a class="reference external" href="http://cython.org/">Cython</a> and calls
<a class="reference external" href="https://www.gnu.org/software/gsl/">GSL</a> functions. This is just to
show that you could be calling some other <code class="docutils literal notranslate"><span class="pre">C</span></code> library that you need.
In this example, the model parameters are all packed into a
list/array/tuple called <code class="docutils literal notranslate"><span class="pre">theta</span></code>.</p>
<p>Let’s also define a <em>really-complicated</em>™ log-likelihood function (a
Normal log-likelihood that ignores the normalisation), which takes in
the list/array/tuple of model parameter values <code class="docutils literal notranslate"><span class="pre">theta</span></code>, the points at
which to calculate the model <code class="docutils literal notranslate"><span class="pre">x</span></code>, the vector of “observed” data points
<code class="docutils literal notranslate"><span class="pre">data</span></code>, and the standard deviation of the noise in the data <code class="docutils literal notranslate"><span class="pre">sigma</span></code>.
This log-likelihood function calls the <em>super-complicated</em>™ model
function.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [2]:
</pre></div>
</div>
<div class="input_area highlight-cython notranslate"><div class="highlight"><pre>
<span></span><span class="o">%%</span><span class="n">cython</span> <span class="o">-</span><span class="n">I</span><span class="o">/</span><span class="n">usr</span><span class="o">/</span><span class="k">include</span> <span class="o">-</span><span class="n">L</span><span class="o">/</span><span class="n">usr</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">gnu</span> <span class="o">-</span><span class="n">lgsl</span> <span class="o">-</span><span class="n">lgslcblas</span> <span class="o">-</span><span class="n">lm</span>

<span class="k">import</span> <span class="nn">cython</span>
<span class="k">cimport</span> <span class="nn">cython</span>

<span class="k">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="k">cimport</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c">### STUFF FOR USING GSL (FEEL FREE TO IGNORE!) ###</span>

<span class="c"># declare GSL vector structure and functions</span>
<span class="k">cdef</span> <span class="kr">extern</span> <span class="k">from</span> <span class="s">&quot;gsl/gsl_block.h&quot;</span><span class="p">:</span>
    <span class="k">cdef</span> <span class="k">struct</span> <span class="nf">gsl_block</span><span class="p">:</span>
        <span class="n">size_t</span> <span class="n">size</span>
        <span class="n">double</span> <span class="o">*</span> <span class="n">data</span>

<span class="k">cdef</span> <span class="kr">extern</span> <span class="k">from</span> <span class="s">&quot;gsl/gsl_vector.h&quot;</span><span class="p">:</span>
    <span class="k">cdef</span> <span class="k">struct</span> <span class="nf">gsl_vector</span><span class="p">:</span>
        <span class="n">size_t</span> <span class="n">size</span>
        <span class="n">size_t</span> <span class="n">stride</span>
        <span class="n">double</span> <span class="o">*</span> <span class="n">data</span>
        <span class="n">gsl_block</span> <span class="o">*</span> <span class="n">block</span>
        <span class="nb">int</span> <span class="n">owner</span>

    <span class="k">ctypedef</span> <span class="k">struct</span> <span class="nc">gsl_vector_view</span><span class="p">:</span>
        <span class="n">gsl_vector</span> <span class="n">vector</span>

    <span class="nb">int</span> <span class="n">gsl_vector_scale</span> <span class="p">(</span><span class="n">gsl_vector</span> <span class="o">*</span> <span class="n">a</span><span class="p">,</span> <span class="n">const</span> <span class="n">double</span> <span class="n">x</span><span class="p">)</span> <span class="k">nogil</span>
    <span class="nb">int</span> <span class="n">gsl_vector_add_constant</span> <span class="p">(</span><span class="n">gsl_vector</span> <span class="o">*</span> <span class="n">a</span><span class="p">,</span> <span class="n">const</span> <span class="n">double</span> <span class="n">x</span><span class="p">)</span> <span class="k">nogil</span>
    <span class="n">gsl_vector_view</span> <span class="n">gsl_vector_view_array</span> <span class="p">(</span><span class="n">double</span> <span class="o">*</span> <span class="n">base</span><span class="p">,</span> <span class="n">size_t</span> <span class="n">n</span><span class="p">)</span> <span class="k">nogil</span>

<span class="c">###################################################</span>


<span class="c"># define your super-complicated model that uses loads of external codes</span>
<span class="k">cpdef</span> <span class="nf">my_model</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float64_t</span><span class="p">,</span> <span class="n">ndim</span><span class="o">=</span><span class="mf">1</span><span class="p">]</span> <span class="n">x</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A straight line!</span>

<span class="sd">    Note:</span>
<span class="sd">        This function could simply be:</span>

<span class="sd">            m, c = thetha</span>
<span class="sd">            return m*x + x</span>

<span class="sd">        but I&#39;ve made it more complicated for demonstration purposes</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">theta</span>  <span class="c"># unpack line gradient and y-intercept</span>

    <span class="k">cdef</span> <span class="kt">size_t</span> <span class="nf">length</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c"># length of x</span>

    <span class="k">cdef</span> <span class="kt">np</span>.<span class="kt">ndarray</span> <span class="nf">line</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c"># make copy of x vector</span>
    <span class="k">cdef</span> <span class="kt">gsl_vector_view</span> <span class="nf">lineview</span>      <span class="c"># create a view of the vector</span>
    <span class="n">lineview</span> <span class="o">=</span> <span class="n">gsl_vector_view_array</span><span class="p">(</span><span class="o">&lt;</span><span class="n">double</span> <span class="o">*&gt;</span><span class="n">line</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">length</span><span class="p">)</span>

    <span class="c"># multiply x by m</span>
    <span class="n">gsl_vector_scale</span><span class="p">(</span><span class="o">&amp;</span><span class="n">lineview</span><span class="o">.</span><span class="n">vector</span><span class="p">,</span> <span class="p">&lt;</span><span class="kt">double</span><span class="p">&gt;</span><span class="n">m</span><span class="p">)</span>

    <span class="c"># add c</span>
    <span class="n">gsl_vector_add_constant</span><span class="p">(</span><span class="o">&amp;</span><span class="n">lineview</span><span class="o">.</span><span class="n">vector</span><span class="p">,</span> <span class="p">&lt;</span><span class="kt">double</span><span class="p">&gt;</span><span class="n">c</span><span class="p">)</span>

    <span class="c"># return the numpy array</span>
    <span class="k">return</span> <span class="n">line</span>


<span class="c"># define your really-complicated likelihood function that uses loads of external codes</span>
<span class="k">cpdef</span> <span class="nf">my_loglike</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float64_t</span><span class="p">,</span> <span class="n">ndim</span><span class="o">=</span><span class="mf">1</span><span class="p">]</span> <span class="n">x</span><span class="p">,</span>
                 <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float64_t</span><span class="p">,</span> <span class="n">ndim</span><span class="o">=</span><span class="mf">1</span><span class="p">]</span> <span class="n">data</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A Gaussian log-likelihood function for a model with parameters given in theta</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">my_model</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

    <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="mf">0.5</span><span class="o">/</span><span class="n">sigma</span><span class="o">**</span><span class="mf">2</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">data</span> <span class="o">-</span> <span class="n">model</span><span class="p">)</span><span class="o">**</span><span class="mf">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Now, as things are, if we wanted to sample from this log-likelihood
function, using certain prior distributions for the model parameters
(gradient and y-intercept) using PyMC3, we might try something like this
(using a <a class="reference external" href="https://docs.pymc.io/prob_dists.html#custom-distributions">PyMC3
DensityDist</a>):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pymc3</span> <span class="kn">as</span> <span class="nn">pm</span>

<span class="c1"># create/read in our &quot;data&quot; (I&#39;ll show this in the real example below)</span>
<span class="n">x</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">data</span> <span class="o">=</span> <span class="o">...</span>

<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">():</span>
    <span class="c1"># set priors on model gradient and y-intercept</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=-</span><span class="mf">10.</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">10.</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=-</span><span class="mf">10.</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">10.</span><span class="p">)</span>

    <span class="c1"># create custom distribution</span>
    <span class="n">pm</span><span class="o">.</span><span class="n">DensityDist</span><span class="p">(</span><span class="s1">&#39;likelihood&#39;</span><span class="p">,</span> <span class="n">my_loglike</span><span class="p">,</span>
                   <span class="n">observed</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;theta&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">c</span><span class="p">),</span> <span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="s1">&#39;data&#39;</span><span class="p">:</span> <span class="n">data</span><span class="p">,</span> <span class="s1">&#39;sigma&#39;</span><span class="p">:</span> <span class="n">sigma</span><span class="p">})</span>

    <span class="c1"># sample from the distribution</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
<p>But, this will give an error like:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="ne">ValueError</span><span class="p">:</span> <span class="n">setting</span> <span class="n">an</span> <span class="n">array</span> <span class="n">element</span> <span class="k">with</span> <span class="n">a</span> <span class="n">sequence</span><span class="o">.</span>
</pre></div>
</div>
<p>This is because <code class="docutils literal notranslate"><span class="pre">m</span></code> and <code class="docutils literal notranslate"><span class="pre">c</span></code> are Theano tensor-type objects.</p>
<p>So, what we actually need to do is create a <a class="reference external" href="http://deeplearning.net/software/theano/extending/extending_theano.html">Theano
Op</a>.
This will be a new class that wraps our log-likelihood function (or just
our model function, if that is all that is required) into something that
can take in Theano tensor objects, but internally can cast them as
floating point values that can be passed to our log-likelihood function.
We will do this below, initially without defining a <a class="reference external" href="http://deeplearning.net/software/theano/extending/op.html#grad">grad()
method</a>
for the Op.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># define a theano Op for our likelihood function</span>
<span class="k">class</span> <span class="nc">LogLike</span><span class="p">(</span><span class="n">tt</span><span class="o">.</span><span class="n">Op</span><span class="p">):</span>

    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Specify what type of object will be passed and returned to the Op when it is</span>
<span class="sd">    called. In our case we will be passing it a vector of values (the parameters</span>
<span class="sd">    that define our model) and returning a single &quot;scalar&quot; value (the</span>
<span class="sd">    log-likelihood)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">itypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">tt</span><span class="o">.</span><span class="n">dvector</span><span class="p">]</span> <span class="c1"># expects a vector of parameter values when called</span>
    <span class="n">otypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">tt</span><span class="o">.</span><span class="n">dscalar</span><span class="p">]</span> <span class="c1"># outputs a single scalar value (the log likelihood)</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loglike</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialise the Op with various things that our log-likelihood function</span>
<span class="sd">        requires. Below are the things that are needed in this particular</span>
<span class="sd">        example.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        loglike:</span>
<span class="sd">            The log-likelihood (or whatever) function we&#39;ve defined</span>
<span class="sd">        data:</span>
<span class="sd">            The &quot;observed&quot; data that our log-likelihood function takes in</span>
<span class="sd">        x:</span>
<span class="sd">            The dependent variable (aka &#39;x&#39;) that our model requires</span>
<span class="sd">        sigma:</span>
<span class="sd">            The noise standard deviation that our function requires.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># add inputs as class attributes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span> <span class="o">=</span> <span class="n">loglike</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span>

    <span class="k">def</span> <span class="nf">perform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
        <span class="c1"># the method that is used when calling the Op</span>
        <span class="n">theta</span><span class="p">,</span> <span class="o">=</span> <span class="n">inputs</span>  <span class="c1"># this will contain my variables</span>

        <span class="c1"># call the log-likelihood function</span>
        <span class="n">logl</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span><span class="p">)</span>

        <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">logl</span><span class="p">)</span> <span class="c1"># output the log-likelihood</span>
</pre></div>
</div>
</div>
<p>Now, let’s use this Op to repeat the example shown above. To do this
let’s create some data containing a straight line with additive Gaussian
noise (with a mean of zero and a standard deviation of <code class="docutils literal notranslate"><span class="pre">sigma</span></code>). For
simplicity we set
<a class="reference external" href="https://docs.pymc.io/api/distributions/continuous.html#pymc3.distributions.continuous.Uniform">uniform</a>
prior distributions on the gradient and y-intercept. As we’ve not set
the <code class="docutils literal notranslate"><span class="pre">grad()</span></code> method of the Op PyMC3 will not be able to use the
gradient-based samplers, so will fall back to using the
<a class="reference external" href="https://docs.pymc.io/api/inference.html#module-pymc3.step_methods.slicer">Slice</a>
sampler.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># set up our data</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># number of data points</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="mf">1.</span>  <span class="c1"># standard deviation of noise</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>

<span class="n">mtrue</span> <span class="o">=</span> <span class="mf">0.4</span>  <span class="c1"># true gradient</span>
<span class="n">ctrue</span> <span class="o">=</span> <span class="mf">3.</span>   <span class="c1"># true y-intercept</span>

<span class="n">truemodel</span> <span class="o">=</span> <span class="n">my_model</span><span class="p">([</span><span class="n">mtrue</span><span class="p">,</span> <span class="n">ctrue</span><span class="p">],</span> <span class="n">x</span><span class="p">)</span>

<span class="c1"># make data</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">716742</span><span class="p">)</span>  <span class="c1"># set random seed, so the data is reproducible each time</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">sigma</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">)</span> <span class="o">+</span> <span class="n">truemodel</span>

<span class="n">ndraws</span> <span class="o">=</span> <span class="mi">3000</span>  <span class="c1"># number of draws from the distribution</span>
<span class="n">nburn</span> <span class="o">=</span> <span class="mi">1000</span>   <span class="c1"># number of &quot;burn-in points&quot; (which we&#39;ll discard)</span>

<span class="c1"># create our Op</span>
<span class="n">logl</span> <span class="o">=</span> <span class="n">LogLike</span><span class="p">(</span><span class="n">my_loglike</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>

<span class="c1"># use PyMC3 to sampler from log-likelihood</span>
<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">():</span>
    <span class="c1"># uniform priors on m and c</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=-</span><span class="mf">10.</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">10.</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=-</span><span class="mf">10.</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">10.</span><span class="p">)</span>

    <span class="c1"># convert m and c to a tensor vector</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">tt</span><span class="o">.</span><span class="n">as_tensor_variable</span><span class="p">([</span><span class="n">m</span><span class="p">,</span> <span class="n">c</span><span class="p">])</span>

    <span class="c1"># use a DensityDist (use a lamdba function to &quot;call&quot; the Op)</span>
    <span class="n">pm</span><span class="o">.</span><span class="n">DensityDist</span><span class="p">(</span><span class="s1">&#39;likelihood&#39;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">logl</span><span class="p">(</span><span class="n">v</span><span class="p">),</span> <span class="n">observed</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;v&#39;</span><span class="p">:</span> <span class="n">theta</span><span class="p">})</span>

    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">ndraws</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="n">nburn</span><span class="p">,</span> <span class="n">discard_tuned_samples</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># plot the traces</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">traceplot</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">lines</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;m&#39;</span><span class="p">:</span> <span class="n">mtrue</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">:</span> <span class="n">ctrue</span><span class="p">})</span>

<span class="c1"># put the chains in an array (for later!)</span>
<span class="n">samples_pymc3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">trace</span><span class="p">[</span><span class="s1">&#39;m&#39;</span><span class="p">],</span> <span class="n">trace</span><span class="p">[</span><span class="s1">&#39;c&#39;</span><span class="p">]))</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="stderr output_area docutils container">
<div class="highlight"><pre>
Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Initializing NUTS failed. Falling back to elementwise auto-assignment.
Multiprocess sampling (2 chains in 2 jobs)
CompoundStep
&gt;Slice: [c]
&gt;Slice: [m]
Sampling 2 chains: 100%|██████████| 8000/8000 [00:04&lt;00:00, 1928.75draws/s]
The number of effective samples is smaller than 25% for some parameters.
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_blackbox_external_likelihood_7_1.png" src="../_images/notebooks_blackbox_external_likelihood_7_1.png" />
</div>
</div>
<p>What if we wanted to use NUTS or HMC? If we knew the analytical
derivatives of the model/likelihood function then we could add a <a class="reference external" href="http://deeplearning.net/software/theano/extending/op.html#grad">grad()
method</a>
to the Op using that analytical form.</p>
<p>But, what if we don’t know the analytical form. If our model/likelihood
is purely Python and made up of standard maths operators and Numpy
functions, then the <a class="reference external" href="https://github.com/HIPS/autograd">autograd</a>
module could potentially be used to find gradients (also, see
<a class="reference external" href="https://github.com/ActiveState/code/blob/master/recipes/Python/580610_Auto_differentiation/recipe-580610.py">here</a>
for a nice Python example of automatic differentiation). But, if our
model/likelihood truely is a “black box” then we can just use the
good-old-fashioned <a class="reference external" href="https://en.wikipedia.org/wiki/Finite_difference">finite
difference</a> to find
the gradients - this can be slow, especially if there are a large number
of variables, or the model takes a long time to evaluate. Below, a
function to find gradients has been defined that uses the finite
difference (the central difference) - it uses an iterative method with
successively smaller interval sizes to check that the gradient
converges. But, you could do something far simpler and just use, for
example, the SciPy
<a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.approx_fprime.html">approx_fprime</a>
function. Here, the gradient function is defined in Cython for speed,
but if the function it evaluates to find the gradients is the
performance bottle neck then having this as a pure Python function may
not make a significant speed difference.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [5]:
</pre></div>
</div>
<div class="input_area highlight-cython notranslate"><div class="highlight"><pre>
<span></span><span class="o">%%</span><span class="n">cython</span>

<span class="k">import</span> <span class="nn">cython</span>
<span class="k">cimport</span> <span class="nn">cython</span>

<span class="k">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="k">cimport</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">import</span> <span class="nn">warnings</span>

<span class="k">def</span> <span class="nf">gradients</span><span class="p">(</span><span class="n">vals</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">releps</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">abseps</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">mineps</span><span class="o">=</span><span class="mf">1e-9</span><span class="p">,</span> <span class="n">reltol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
              <span class="n">epsscale</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the partial derivatives of a function at a set of values. The</span>
<span class="sd">    derivatives are calculated using the central difference, using an iterative</span>
<span class="sd">    method to check that the values converge as step size decreases.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    vals: array_like</span>
<span class="sd">        A set of values, that are passed to a function, at which to calculate</span>
<span class="sd">        the gradient of that function</span>
<span class="sd">    func:</span>
<span class="sd">        A function that takes in an array of values.</span>
<span class="sd">    releps: float, array_like, 1e-3</span>
<span class="sd">        The initial relative step size for calculating the derivative.</span>
<span class="sd">    abseps: float, array_like, None</span>
<span class="sd">        The initial absolute step size for calculating the derivative.</span>
<span class="sd">        This overrides `releps` if set.</span>
<span class="sd">        `releps` is set then that is used.</span>
<span class="sd">    mineps: float, 1e-9</span>
<span class="sd">        The minimum relative step size at which to stop iterations if no</span>
<span class="sd">        convergence is achieved.</span>
<span class="sd">    epsscale: float, 0.5</span>
<span class="sd">        The factor by which releps if scaled in each iteration.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    grads: array_like</span>
<span class="sd">        An array of gradients for each non-fixed value.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">grads</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vals</span><span class="p">))</span>

    <span class="c"># maximum number of times the gradient can change sign</span>
    <span class="n">flipflopmax</span> <span class="o">=</span> <span class="mf">10.</span>

    <span class="c"># set steps</span>
    <span class="k">if</span> <span class="n">abseps</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">releps</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
            <span class="n">eps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">vals</span><span class="p">)</span><span class="o">*</span><span class="n">releps</span>
            <span class="n">eps</span><span class="p">[</span><span class="n">eps</span> <span class="o">==</span> <span class="mf">0.</span><span class="p">]</span> <span class="o">=</span> <span class="n">releps</span>  <span class="c"># if any values are zero set eps to releps</span>
            <span class="n">teps</span> <span class="o">=</span> <span class="n">releps</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vals</span><span class="p">))</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">releps</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">releps</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vals</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&quot;Problem with input relative step sizes&quot;</span><span class="p">)</span>
            <span class="n">eps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">vals</span><span class="p">),</span> <span class="n">releps</span><span class="p">)</span>
            <span class="n">eps</span><span class="p">[</span><span class="n">eps</span> <span class="o">==</span> <span class="mf">0.</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">releps</span><span class="p">)[</span><span class="n">eps</span> <span class="o">==</span> <span class="mf">0.</span><span class="p">]</span>
            <span class="n">teps</span> <span class="o">=</span> <span class="n">releps</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s">&quot;Relative step sizes are not a recognised type!&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">abseps</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
            <span class="n">eps</span> <span class="o">=</span> <span class="n">abseps</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vals</span><span class="p">))</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">abseps</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">abseps</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vals</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&quot;Problem with input absolute step sizes&quot;</span><span class="p">)</span>
            <span class="n">eps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">abseps</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s">&quot;Absolute step sizes are not a recognised type!&quot;</span><span class="p">)</span>
        <span class="n">teps</span> <span class="o">=</span> <span class="n">eps</span>

    <span class="c"># for each value in vals calculate the gradient</span>
    <span class="n">count</span> <span class="o">=</span> <span class="mf">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vals</span><span class="p">)):</span>
        <span class="c"># initial parameter diffs</span>
        <span class="n">leps</span> <span class="o">=</span> <span class="n">eps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">cureps</span> <span class="o">=</span> <span class="n">teps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

        <span class="n">flipflop</span> <span class="o">=</span> <span class="mf">0</span>

        <span class="c"># get central finite difference</span>
        <span class="n">fvals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">vals</span><span class="p">)</span>
        <span class="n">bvals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">vals</span><span class="p">)</span>

        <span class="c"># central difference</span>
        <span class="n">fvals</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">leps</span>  <span class="c"># change forwards distance to half eps</span>
        <span class="n">bvals</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">leps</span>  <span class="c"># change backwards distance to half eps</span>
        <span class="n">cdiff</span> <span class="o">=</span> <span class="p">(</span><span class="n">func</span><span class="p">(</span><span class="n">fvals</span><span class="p">)</span><span class="o">-</span><span class="n">func</span><span class="p">(</span><span class="n">bvals</span><span class="p">))</span><span class="o">/</span><span class="n">leps</span>

        <span class="k">while</span> <span class="mf">1</span><span class="p">:</span>
            <span class="n">fvals</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">leps</span>  <span class="c"># remove old step</span>
            <span class="n">bvals</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">leps</span>

            <span class="c"># change the difference by a factor of two</span>
            <span class="n">cureps</span> <span class="o">*=</span> <span class="n">epsscale</span>
            <span class="k">if</span> <span class="n">cureps</span> <span class="o">&lt;</span> <span class="n">mineps</span> <span class="ow">or</span> <span class="n">flipflop</span> <span class="o">&gt;</span> <span class="n">flipflopmax</span><span class="p">:</span>
                <span class="c"># if no convergence set flat derivative (TODO: check if there is a better thing to do instead)</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s">&quot;Derivative calculation did not converge: setting flat derivative.&quot;</span><span class="p">)</span>
                <span class="n">grads</span><span class="p">[</span><span class="n">count</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.</span>
                <span class="k">break</span>
            <span class="n">leps</span> <span class="o">*=</span> <span class="n">epsscale</span>

            <span class="c"># central difference</span>
            <span class="n">fvals</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">leps</span>  <span class="c"># change forwards distance to half eps</span>
            <span class="n">bvals</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">leps</span>  <span class="c"># change backwards distance to half eps</span>
            <span class="n">cdiffnew</span> <span class="o">=</span> <span class="p">(</span><span class="n">func</span><span class="p">(</span><span class="n">fvals</span><span class="p">)</span><span class="o">-</span><span class="n">func</span><span class="p">(</span><span class="n">bvals</span><span class="p">))</span><span class="o">/</span><span class="n">leps</span>

            <span class="k">if</span> <span class="n">cdiffnew</span> <span class="o">==</span> <span class="n">cdiff</span><span class="p">:</span>
                <span class="n">grads</span><span class="p">[</span><span class="n">count</span><span class="p">]</span> <span class="o">=</span> <span class="n">cdiff</span>
                <span class="k">break</span>

            <span class="c"># check whether previous diff and current diff are the same within reltol</span>
            <span class="n">rat</span> <span class="o">=</span> <span class="p">(</span><span class="n">cdiff</span><span class="o">/</span><span class="n">cdiffnew</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">rat</span><span class="p">)</span> <span class="ow">and</span> <span class="n">rat</span> <span class="o">&gt;</span> <span class="mf">0.</span><span class="p">:</span>
                <span class="c"># gradient has not changed sign</span>
                <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="mf">1.</span><span class="o">-</span><span class="n">rat</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">reltol</span><span class="p">:</span>
                    <span class="n">grads</span><span class="p">[</span><span class="n">count</span><span class="p">]</span> <span class="o">=</span> <span class="n">cdiffnew</span>
                    <span class="k">break</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">cdiff</span> <span class="o">=</span> <span class="n">cdiffnew</span>
                    <span class="k">continue</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">cdiff</span> <span class="o">=</span> <span class="n">cdiffnew</span>
                <span class="n">flipflop</span> <span class="o">+=</span> <span class="mf">1</span>
                <span class="k">continue</span>

        <span class="n">count</span> <span class="o">+=</span> <span class="mf">1</span>

    <span class="k">return</span> <span class="n">grads</span>
</pre></div>
</div>
</div>
<p>So, now we can just redefine our Op with a <code class="docutils literal notranslate"><span class="pre">grad()</span></code> method, right?</p>
<p>It’s not quite so simple! The <code class="docutils literal notranslate"><span class="pre">grad()</span></code> method itself requires that its
inputs are Theano tensor variables, whereas our <code class="docutils literal notranslate"><span class="pre">gradients</span></code> function
above, like our <code class="docutils literal notranslate"><span class="pre">my_loglike</span></code> function, wants a list of floating point
values. So, we need to define another Op that calculates the gradients.
Below, I define a new version of the <code class="docutils literal notranslate"><span class="pre">LogLike</span></code> Op, called
<code class="docutils literal notranslate"><span class="pre">LogLikeWithGrad</span></code> this time, that has a <code class="docutils literal notranslate"><span class="pre">grad()</span></code> method. This is
followed by anothor Op called <code class="docutils literal notranslate"><span class="pre">LogLikeGrad</span></code> that, when called with a
vector of Theano tensor variables, returns another vector of values that
are the gradients (i.e., the
<a class="reference external" href="https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant">Jacobian</a>)
of our log-likelihood function at those values. Note that the <code class="docutils literal notranslate"><span class="pre">grad()</span></code>
method itself does not return the gradients directly, but instead
returns the
<a class="reference external" href="https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant">Jacobian</a>-vector
product (you can hopefully just copy what I’ve done and not worry about
what this means too much!).</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># define a theano Op for our likelihood function</span>
<span class="k">class</span> <span class="nc">LogLikeWithGrad</span><span class="p">(</span><span class="n">tt</span><span class="o">.</span><span class="n">Op</span><span class="p">):</span>

    <span class="n">itypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">tt</span><span class="o">.</span><span class="n">dvector</span><span class="p">]</span> <span class="c1"># expects a vector of parameter values when called</span>
    <span class="n">otypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">tt</span><span class="o">.</span><span class="n">dscalar</span><span class="p">]</span> <span class="c1"># outputs a single scalar value (the log likelihood)</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loglike</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialise with various things that the function requires. Below</span>
<span class="sd">        are the things that are needed in this particular example.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        loglike:</span>
<span class="sd">            The log-likelihood (or whatever) function we&#39;ve defined</span>
<span class="sd">        data:</span>
<span class="sd">            The &quot;observed&quot; data that our log-likelihood function takes in</span>
<span class="sd">        x:</span>
<span class="sd">            The dependent variable (aka &#39;x&#39;) that our model requires</span>
<span class="sd">        sigma:</span>
<span class="sd">            The noise standard deviation that out function requires.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># add inputs as class attributes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span> <span class="o">=</span> <span class="n">loglike</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span>

        <span class="c1"># initialise the gradient Op (below)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logpgrad</span> <span class="o">=</span> <span class="n">LogLikeGrad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">perform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
        <span class="c1"># the method that is used when calling the Op</span>
        <span class="n">theta</span><span class="p">,</span> <span class="o">=</span> <span class="n">inputs</span>  <span class="c1"># this will contain my variables</span>

        <span class="c1"># call the log-likelihood function</span>
        <span class="n">logl</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span><span class="p">)</span>

        <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">logl</span><span class="p">)</span> <span class="c1"># output the log-likelihood</span>

    <span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">g</span><span class="p">):</span>
        <span class="c1"># the method that calculates the gradients - it actually returns the</span>
        <span class="c1"># vector-Jacobian product - g[0] is a vector of parameter values</span>
        <span class="n">theta</span><span class="p">,</span> <span class="o">=</span> <span class="n">inputs</span>  <span class="c1"># our parameters</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">g</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">logpgrad</span><span class="p">(</span><span class="n">theta</span><span class="p">)]</span>


<span class="k">class</span> <span class="nc">LogLikeGrad</span><span class="p">(</span><span class="n">tt</span><span class="o">.</span><span class="n">Op</span><span class="p">):</span>

    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This Op will be called with a vector of values and also return a vector of</span>
<span class="sd">    values - the gradients in each dimension.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">itypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">tt</span><span class="o">.</span><span class="n">dvector</span><span class="p">]</span>
    <span class="n">otypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">tt</span><span class="o">.</span><span class="n">dvector</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loglike</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialise with various things that the function requires. Below</span>
<span class="sd">        are the things that are needed in this particular example.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        loglike:</span>
<span class="sd">            The log-likelihood (or whatever) function we&#39;ve defined</span>
<span class="sd">        data:</span>
<span class="sd">            The &quot;observed&quot; data that our log-likelihood function takes in</span>
<span class="sd">        x:</span>
<span class="sd">            The dependent variable (aka &#39;x&#39;) that our model requires</span>
<span class="sd">        sigma:</span>
<span class="sd">            The noise standard deviation that out function requires.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># add inputs as class attributes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span> <span class="o">=</span> <span class="n">loglike</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span>

    <span class="k">def</span> <span class="nf">perform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
        <span class="n">theta</span><span class="p">,</span> <span class="o">=</span> <span class="n">inputs</span>

        <span class="c1"># define version of likelihood function to pass to derivative function</span>
        <span class="k">def</span> <span class="nf">lnlike</span><span class="p">(</span><span class="n">values</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span><span class="p">)</span>

        <span class="c1"># calculate gradients</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="n">gradients</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">lnlike</span><span class="p">)</span>

        <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">grads</span>
</pre></div>
</div>
</div>
<p>Now, let’s re-run PyMC3 with our new “grad”-ed Op. This time it will be
able to automatically use NUTS.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># create our Op</span>
<span class="n">logl</span> <span class="o">=</span> <span class="n">LogLikeWithGrad</span><span class="p">(</span><span class="n">my_loglike</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>

<span class="c1"># use PyMC3 to sampler from log-likelihood</span>
<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">opmodel</span><span class="p">:</span>
    <span class="c1"># uniform priors on m and c</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=-</span><span class="mf">10.</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">10.</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=-</span><span class="mf">10.</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">10.</span><span class="p">)</span>

    <span class="c1"># convert m and c to a tensor vector</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">tt</span><span class="o">.</span><span class="n">as_tensor_variable</span><span class="p">([</span><span class="n">m</span><span class="p">,</span> <span class="n">c</span><span class="p">])</span>

    <span class="c1"># use a DensityDist</span>
    <span class="n">pm</span><span class="o">.</span><span class="n">DensityDist</span><span class="p">(</span><span class="s1">&#39;likelihood&#39;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">logl</span><span class="p">(</span><span class="n">v</span><span class="p">),</span> <span class="n">observed</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;v&#39;</span><span class="p">:</span> <span class="n">theta</span><span class="p">})</span>

    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">ndraws</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="n">nburn</span><span class="p">,</span> <span class="n">discard_tuned_samples</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># plot the traces</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">traceplot</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">lines</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;m&#39;</span><span class="p">:</span> <span class="n">mtrue</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">:</span> <span class="n">ctrue</span><span class="p">})</span>

<span class="c1"># put the chains in an array (for later!)</span>
<span class="n">samples_pymc3_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">trace</span><span class="p">[</span><span class="s1">&#39;m&#39;</span><span class="p">],</span> <span class="n">trace</span><span class="p">[</span><span class="s1">&#39;c&#39;</span><span class="p">]))</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="stderr output_area docutils container">
<div class="highlight"><pre>
Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (2 chains in 2 jobs)
NUTS: [c, m]
Sampling 2 chains: 100%|██████████| 8000/8000 [00:08&lt;00:00, 898.88draws/s]
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_blackbox_external_likelihood_13_1.png" src="../_images/notebooks_blackbox_external_likelihood_13_1.png" />
</div>
</div>
<p>Now, finally, just to check things actually worked as we might expect,
let’s do the same thing purely using PyMC3 distributions (because in
this simple example we can!)</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">pymodel</span><span class="p">:</span>
    <span class="c1"># uniform priors on m and c</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=-</span><span class="mf">10.</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">10.</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=-</span><span class="mf">10.</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">10.</span><span class="p">)</span>

    <span class="c1"># convert m and c to a tensor vector</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">tt</span><span class="o">.</span><span class="n">as_tensor_variable</span><span class="p">([</span><span class="n">m</span><span class="p">,</span> <span class="n">c</span><span class="p">])</span>

    <span class="c1"># use a Normal distribution</span>
    <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;likelihood&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="p">(</span><span class="n">m</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="n">c</span><span class="p">),</span> <span class="n">sd</span> <span class="o">=</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>

    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">ndraws</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="n">nburn</span><span class="p">,</span> <span class="n">discard_tuned_samples</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># plot the traces</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">traceplot</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">lines</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;m&#39;</span><span class="p">:</span> <span class="n">mtrue</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">:</span> <span class="n">ctrue</span><span class="p">})</span>

<span class="c1"># put the chains in an array (for later!)</span>
<span class="n">samples_pymc3_3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">trace</span><span class="p">[</span><span class="s1">&#39;m&#39;</span><span class="p">],</span> <span class="n">trace</span><span class="p">[</span><span class="s1">&#39;c&#39;</span><span class="p">]))</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="stderr output_area docutils container">
<div class="highlight"><pre>
Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (2 chains in 2 jobs)
NUTS: [c, m]
Sampling 2 chains: 100%|██████████| 8000/8000 [00:03&lt;00:00, 2049.77draws/s]
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_blackbox_external_likelihood_15_1.png" src="../_images/notebooks_blackbox_external_likelihood_15_1.png" />
</div>
</div>
<p>To check that they match let’s plot all the examples together and also
find the autocorrelation lengths.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">FutureWarning</span><span class="p">)</span>  <span class="c1"># supress emcee autocorr FutureWarning</span>

<span class="n">matplotlib</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;font.size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">18</span>

<span class="n">hist2dkwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;plot_datapoints&#39;</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
                <span class="s1">&#39;plot_density&#39;</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
                <span class="s1">&#39;levels&#39;</span><span class="p">:</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)}</span> <span class="c1"># roughly 1 and 2 sigma</span>

<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">]</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Theanp Op (no grad)&#39;</span><span class="p">,</span> <span class="s1">&#39;Theano Op (with grad)&#39;</span><span class="p">,</span> <span class="s1">&#39;Pure PyMC3&#39;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">samples</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="n">samples_pymc3</span><span class="p">,</span> <span class="n">samples_pymc3_2</span><span class="p">,</span> <span class="n">samples_pymc3_3</span><span class="p">]):</span>
    <span class="c1"># get maximum chain autocorrelartion length</span>
    <span class="n">autocorrlen</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">emcee</span><span class="o">.</span><span class="n">autocorr</span><span class="o">.</span><span class="n">integrated_time</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="mi">3</span><span class="p">)));</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Auto-correlation length ({}): {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">autocorrlen</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">fig</span> <span class="o">=</span> <span class="n">corner</span><span class="o">.</span><span class="n">corner</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="sa">r</span><span class="s2">&quot;$m$&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;$c$&quot;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                            <span class="n">hist_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;density&#39;</span><span class="p">:</span> <span class="bp">True</span><span class="p">},</span> <span class="o">**</span><span class="n">hist2dkwargs</span><span class="p">,</span>
                            <span class="n">truths</span><span class="o">=</span><span class="p">[</span><span class="n">mtrue</span><span class="p">,</span> <span class="n">ctrue</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">corner</span><span class="o">.</span><span class="n">corner</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">hist_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;density&#39;</span><span class="p">:</span> <span class="bp">True</span><span class="p">},</span>
                      <span class="n">fig</span><span class="o">=</span><span class="n">fig</span><span class="p">,</span> <span class="o">**</span><span class="n">hist2dkwargs</span><span class="p">)</span>

<span class="n">fig</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Auto-correlation length (Theanp Op (no grad)): 6
Auto-correlation length (Theano Op (with grad)): 3
Auto-correlation length (Pure PyMC3): 3
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_blackbox_external_likelihood_17_1.png" src="../_images/notebooks_blackbox_external_likelihood_17_1.png" />
</div>
</div>
<p>We can now check that the gradient Op works was we expect it to. First,
just create and call the <code class="docutils literal notranslate"><span class="pre">LogLikeGrad</span></code> class, which should return the
gradient directly (note that we have to create a <a class="reference external" href="http://deeplearning.net/software/theano/library/compile/function.html">Theano
function</a>
to convert the output of the Op to an array). Secondly, we call the
gradient from <code class="docutils literal notranslate"><span class="pre">LogLikeWithGrad</span></code> by using the <a class="reference external" href="http://deeplearning.net/software/theano/library/gradient.html#theano.gradient.grad">Theano tensor
gradient</a>
function. Finally, we will check the gradient returned by the PyMC3
model for a Normal distribution, which should be the same as the
log-likelihood function we defined. In all cases we evaluate the
gradients at the true values of the model function (the straight line)
that was created.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># test the gradient Op by direct call</span>
<span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">compute_test_value</span> <span class="o">=</span> <span class="s2">&quot;ignore&quot;</span>
<span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">exception_verbosity</span> <span class="o">=</span> <span class="s2">&quot;high&quot;</span>

<span class="n">var</span> <span class="o">=</span> <span class="n">tt</span><span class="o">.</span><span class="n">dvector</span><span class="p">()</span>
<span class="n">test_grad_op</span> <span class="o">=</span> <span class="n">LogLikeGrad</span><span class="p">(</span><span class="n">my_loglike</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
<span class="n">test_grad_op_func</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">([</span><span class="n">var</span><span class="p">],</span> <span class="n">test_grad_op</span><span class="p">(</span><span class="n">var</span><span class="p">))</span>
<span class="n">grad_vals</span> <span class="o">=</span> <span class="n">test_grad_op_func</span><span class="p">([</span><span class="n">mtrue</span><span class="p">,</span> <span class="n">ctrue</span><span class="p">])</span>

<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Gradient returned by &quot;LogLikeGrad&quot;: {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">grad_vals</span><span class="p">))</span>

<span class="c1"># test the gradient called through LogLikeWithGrad</span>
<span class="n">test_gradded_op</span> <span class="o">=</span> <span class="n">LogLikeWithGrad</span><span class="p">(</span><span class="n">my_loglike</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
<span class="n">test_gradded_op_grad</span> <span class="o">=</span> <span class="n">tt</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">test_gradded_op</span><span class="p">(</span><span class="n">var</span><span class="p">),</span> <span class="n">var</span><span class="p">)</span>
<span class="n">test_gradded_op_grad_func</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">([</span><span class="n">var</span><span class="p">],</span> <span class="n">test_gradded_op_grad</span><span class="p">)</span>
<span class="n">grad_vals_2</span> <span class="o">=</span> <span class="n">test_gradded_op_grad_func</span><span class="p">([</span><span class="n">mtrue</span><span class="p">,</span> <span class="n">ctrue</span><span class="p">])</span>

<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Gradient returned by &quot;LogLikeWithGrad&quot;: {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">grad_vals_2</span><span class="p">))</span>

<span class="c1"># test the gradient that PyMC3 uses for the Normal log likelihood</span>
<span class="n">test_model</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span>
<span class="k">with</span> <span class="n">test_model</span><span class="p">:</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=-</span><span class="mf">10.</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">10.</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=-</span><span class="mf">10.</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">10.</span><span class="p">)</span>

    <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;likelihood&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="p">(</span><span class="n">m</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="n">c</span><span class="p">),</span> <span class="n">sd</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>

    <span class="n">gradfunc</span> <span class="o">=</span> <span class="n">test_model</span><span class="o">.</span><span class="n">logp_dlogp_function</span><span class="p">([</span><span class="n">m</span><span class="p">,</span> <span class="n">c</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
    <span class="n">gradfunc</span><span class="o">.</span><span class="n">set_extra_values</span><span class="p">({</span><span class="s1">&#39;m_interval__&#39;</span><span class="p">:</span> <span class="n">mtrue</span><span class="p">,</span> <span class="s1">&#39;c_interval__&#39;</span><span class="p">:</span> <span class="n">ctrue</span><span class="p">})</span>
    <span class="n">grad_vals_pymc3</span> <span class="o">=</span> <span class="n">gradfunc</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">mtrue</span><span class="p">,</span> <span class="n">ctrue</span><span class="p">]))[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># get dlogp values</span>

<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Gradient returned by PyMC3 &quot;Normal&quot; distribution: {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">grad_vals_pymc3</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Gradient returned by &#34;LogLikeGrad&#34;: [-7.17656625 -1.39486358]
Gradient returned by &#34;LogLikeWithGrad&#34;: [-7.17656625 -1.39486358]
Gradient returned by PyMC3 &#34;Normal&#34; distribution: [-7.17656625 -1.39486358]
</pre></div></div>
</div>
<p>We can also do some
<a class="reference external" href="http://docs.pymc.io/notebooks/profiling.html">profiling</a> of the Op,
as used within a PyMC3 Model, to check performance. First, we’ll profile
using the <code class="docutils literal notranslate"><span class="pre">LogLikeWithGrad</span></code> Op, and then doing the same thing purely
using PyMC3 distributions.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># profile logpt using our Op</span>
<span class="n">opmodel</span><span class="o">.</span><span class="n">profile</span><span class="p">(</span><span class="n">opmodel</span><span class="o">.</span><span class="n">logpt</span><span class="p">)</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="stderr output_area docutils container">
<div class="highlight"><pre>
Function profiling
==================
  Message: /home/matthew/.conda/envs/testing/lib/python3.6/site-packages/pymc3/model.py:909
  Time in 1000 calls to Function.__call__: 4.123712e-02s
  Time in Function.fn.__call__: 2.523708e-02s (61.200%)
  Time in thunks: 2.199244e-02s (53.332%)
  Total compile time: 7.463312e-02s
    Number of Apply nodes: 8
    Theano Optimizer time: 5.898285e-02s
       Theano validate time: 5.080700e-04s
    Theano Linker time (includes C, CUDA code generation/compiling): 4.571915e-03s
       Import time 0.000000e+00s
       Node make_thunk time 4.115105e-03s
           Node Elemwise{Composite{((i0 + Switch(Cast{int8}((GE(i1, i2) * LE(i1, i3))), i4, i5)) - ((i6 * scalar_softplus((-i7))) + i7))}}[(0, 1)](TensorConstant{2.995732273553991}, c, TensorConstant{-10.0}, TensorConstant{10.0}, TensorConstant{-2.995732273553991}, TensorConstant{-inf}, TensorConstant{2.0}, c_interval__) time 8.506775e-04s
           Node Elemwise{Composite{((i0 + Switch(Cast{int8}((GE(i1, i2) * LE(i1, i3))), i4, i5)) - ((i6 * scalar_softplus((-i7))) + i7))}}[(0, 1)](TensorConstant{2.995732273553991}, m, TensorConstant{-10.0}, TensorConstant{10.0}, TensorConstant{-2.995732273553991}, TensorConstant{-inf}, TensorConstant{2.0}, m_interval__) time 8.454323e-04s
           Node Elemwise{Composite{(i0 + (i1 * scalar_sigmoid(i2)))}}(TensorConstant{-10.0}, TensorConstant{20.0}, c_interval__) time 6.031990e-04s
           Node Elemwise{Composite{(i0 + (i1 * scalar_sigmoid(i2)))}}(TensorConstant{-10.0}, TensorConstant{20.0}, m_interval__) time 5.083084e-04s
           Node &lt;__main__.LogLikeWithGrad object at 0x7f1d606e8fd0&gt;(MakeVector{dtype=&#39;float64&#39;}.0) time 3.864765e-04s

Time in all call to theano.grad() 6.874464e-01s
Time since theano import 43.624s
Class
---
&lt;% time&gt; &lt;sum %&gt; &lt;apply time&gt; &lt;time per call&gt; &lt;type&gt; &lt;#call&gt; &lt;#apply&gt; &lt;Class name&gt;
  91.8%    91.8%       0.020s       2.02e-05s     Py    1000       1   __main__.LogLikeWithGrad
   4.5%    96.3%       0.001s       2.48e-07s     C     4000       4   theano.tensor.elemwise.Elemwise
   2.8%    99.1%       0.001s       3.04e-07s     C     2000       2   theano.tensor.opt.MakeVector
   0.9%   100.0%       0.000s       2.06e-07s     C     1000       1   theano.tensor.elemwise.Sum
   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)

Ops
---
&lt;% time&gt; &lt;sum %&gt; &lt;apply time&gt; &lt;time per call&gt; &lt;type&gt; &lt;#call&gt; &lt;#apply&gt; &lt;Op name&gt;
  91.8%    91.8%       0.020s       2.02e-05s     Py    1000        1   &lt;__main__.LogLikeWithGrad object at 0x7f1d606e8fd0&gt;
   2.8%    94.6%       0.001s       3.04e-07s     C     2000        2   MakeVector{dtype=&#39;float64&#39;}
   2.5%    97.1%       0.001s       2.79e-07s     C     2000        2   Elemwise{Composite{(i0 + (i1 * scalar_sigmoid(i2)))}}
   2.0%    99.1%       0.000s       2.16e-07s     C     2000        2   Elemwise{Composite{((i0 + Switch(Cast{int8}((GE(i1, i2) * LE(i1, i3))), i4, i5)) - ((i6 * scalar_softplus((-i7))) + i7))}}[(0, 1)]
   0.9%   100.0%       0.000s       2.06e-07s     C     1000        1   Sum{acc_dtype=float64}
   ... (remaining 0 Ops account for   0.00%(0.00s) of the runtime)

Apply
------
&lt;% time&gt; &lt;sum %&gt; &lt;apply time&gt; &lt;time per call&gt; &lt;#call&gt; &lt;id&gt; &lt;Apply name&gt;
  91.8%    91.8%       0.020s       2.02e-05s   1000     5   &lt;__main__.LogLikeWithGrad object at 0x7f1d606e8fd0&gt;(MakeVector{dtype=&#39;float64&#39;}.0)
   2.0%    93.8%       0.000s       4.42e-07s   1000     1   Elemwise{Composite{(i0 + (i1 * scalar_sigmoid(i2)))}}(TensorConstant{-10.0}, TensorConstant{20.0}, m_interval__)
   1.7%    95.5%       0.000s       3.73e-07s   1000     6   MakeVector{dtype=&#39;float64&#39;}(__logp_m_interval__, __logp_c_interval__, __logp_likelihood)
   1.5%    97.0%       0.000s       3.30e-07s   1000     4   Elemwise{Composite{((i0 + Switch(Cast{int8}((GE(i1, i2) * LE(i1, i3))), i4, i5)) - ((i6 * scalar_softplus((-i7))) + i7))}}[(0, 1)](TensorConstant{2.995732273553991}, m, TensorConstant{-10.0}, TensorConstant{10.0}, TensorConstant{-2.995732273553991}, TensorConstant{-inf}, TensorConstant{2.0}, m_interval__)
   1.1%    98.1%       0.000s       2.35e-07s   1000     2   MakeVector{dtype=&#39;float64&#39;}(m, c)
   0.9%    99.0%       0.000s       2.06e-07s   1000     7   Sum{acc_dtype=float64}(MakeVector{dtype=&#39;float64&#39;}.0)
   0.5%    99.5%       0.000s       1.16e-07s   1000     0   Elemwise{Composite{(i0 + (i1 * scalar_sigmoid(i2)))}}(TensorConstant{-10.0}, TensorConstant{20.0}, c_interval__)
   0.5%   100.0%       0.000s       1.03e-07s   1000     3   Elemwise{Composite{((i0 + Switch(Cast{int8}((GE(i1, i2) * LE(i1, i3))), i4, i5)) - ((i6 * scalar_softplus((-i7))) + i7))}}[(0, 1)](TensorConstant{2.995732273553991}, c, TensorConstant{-10.0}, TensorConstant{10.0}, TensorConstant{-2.995732273553991}, TensorConstant{-inf}, TensorConstant{2.0}, c_interval__)
   ... (remaining 0 Apply instances account for 0.00%(0.00s) of the runtime)

Here are tips to potentially make your code run faster
                 (if you think of new ones, suggest them on the mailing list).
                 Test them first, as they are not guaranteed to always provide a speedup.
  - Try the Theano flag floatX=float32
  - Try installing amdlibm and set the Theano flag lib.amdlibm=True. This speeds up only some Elemwise operation.
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># profile using our PyMC3 distribution</span>
<span class="n">pymodel</span><span class="o">.</span><span class="n">profile</span><span class="p">(</span><span class="n">pymodel</span><span class="o">.</span><span class="n">logpt</span><span class="p">)</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="stderr output_area docutils container">
<div class="highlight"><pre>
Function profiling
==================
  Message: /home/matthew/.conda/envs/testing/lib/python3.6/site-packages/pymc3/model.py:909
  Time in 1000 calls to Function.__call__: 1.982212e-02s
  Time in Function.fn.__call__: 5.885363e-03s (29.691%)
  Time in thunks: 2.546072e-03s (12.845%)
  Total compile time: 1.308653e-01s
    Number of Apply nodes: 11
    Theano Optimizer time: 1.083095e-01s
       Theano validate time: 8.940697e-04s
    Theano Linker time (includes C, CUDA code generation/compiling): 8.928537e-03s
       Import time 1.179457e-03s
       Node make_thunk time 8.301497e-03s
           Node Elemwise{Composite{(i0 * scalar_sigmoid(i1))}}(TensorConstant{20.0}, c_interval__) time 1.806974e-03s
           Node Elemwise{Composite{((i0 + Switch(Cast{int8}((GE((i1 + i2), i1) * LE((i1 + i2), i3))), i4, i5)) - ((i6 * scalar_softplus((-i7))) + i7))}}[(0, 2)](TensorConstant{2.995732273553991}, TensorConstant{-10.0}, Elemwise{Composite{(i0 * scalar_sigmoid(i1))}}.0, TensorConstant{10.0}, TensorConstant{-2.995732273553991}, TensorConstant{-inf}, TensorConstant{2.0}, c_interval__) time 1.499176e-03s
           Node InplaceDimShuffle{x}(Elemwise{Composite{(i0 * scalar_sigmoid(i1))}}.0) time 8.523464e-04s
           Node Elemwise{Composite{((i0 + Switch(Cast{int8}((GE(i1, i2) * LE(i1, i3))), i4, i5)) - ((i6 * scalar_softplus((-i7))) + i7))}}[(0, 1)](TensorConstant{2.995732273553991}, m, TensorConstant{-10.0}, TensorConstant{10.0}, TensorConstant{-2.995732273553991}, TensorConstant{-inf}, TensorConstant{2.0}, m_interval__) time 7.934570e-04s
           Node InplaceDimShuffle{x}(m) time 7.789135e-04s

Time in all call to theano.grad() 6.874464e-01s
Time since theano import 43.954s
Class
---
&lt;% time&gt; &lt;sum %&gt; &lt;apply time&gt; &lt;time per call&gt; &lt;type&gt; &lt;#call&gt; &lt;#apply&gt; &lt;Class name&gt;
  53.3%    53.3%       0.001s       2.26e-07s     C     6000       6   theano.tensor.elemwise.Elemwise
  22.9%    76.2%       0.001s       2.91e-07s     C     2000       2   theano.tensor.elemwise.DimShuffle
  14.1%    90.3%       0.000s       3.58e-07s     C     1000       1   theano.tensor.opt.MakeVector
   9.7%   100.0%       0.000s       1.23e-07s     C     2000       2   theano.tensor.elemwise.Sum
   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)

Ops
---
&lt;% time&gt; &lt;sum %&gt; &lt;apply time&gt; &lt;time per call&gt; &lt;type&gt; &lt;#call&gt; &lt;#apply&gt; &lt;Op name&gt;
  22.9%    22.9%       0.001s       2.91e-07s     C     2000        2   InplaceDimShuffle{x}
  14.1%    37.0%       0.000s       3.58e-07s     C     1000        1   MakeVector{dtype=&#39;float64&#39;}
  11.5%    48.5%       0.000s       2.93e-07s     C     1000        1   Elemwise{Composite{(i0 + (-sqr((i1 - ((i2 * i3) + i4)))))}}
  11.3%    59.8%       0.000s       2.88e-07s     C     1000        1   Elemwise{Composite{(i0 + (i1 * scalar_sigmoid(i2)))}}
  10.8%    70.6%       0.000s       2.75e-07s     C     1000        1   Elemwise{Composite{((i0 + Switch(Cast{int8}((GE((i1 + i2), i1) * LE((i1 + i2), i3))), i4, i5)) - ((i6 * scalar_softplus((-i7))) + i7))}}[(0, 2)]
   9.7%    80.3%       0.000s       2.47e-07s     C     1000        1   Elemwise{Composite{((i0 + Switch(Cast{int8}((GE(i1, i2) * LE(i1, i3))), i4, i5)) - ((i6 * scalar_softplus((-i7))) + i7))}}[(0, 1)]
   9.7%    90.0%       0.000s       1.23e-07s     C     2000        2   Sum{acc_dtype=float64}
   6.0%    96.0%       0.000s       1.52e-07s     C     1000        1   Elemwise{Composite{(i0 * scalar_sigmoid(i1))}}
   4.0%   100.0%       0.000s       1.03e-07s     C     1000        1   Elemwise{Mul}[(0, 1)]
   ... (remaining 0 Ops account for   0.00%(0.00s) of the runtime)

Apply
------
&lt;% time&gt; &lt;sum %&gt; &lt;apply time&gt; &lt;time per call&gt; &lt;#call&gt; &lt;id&gt; &lt;Apply name&gt;
  14.1%    14.1%       0.000s       3.58e-07s   1000     9   MakeVector{dtype=&#39;float64&#39;}(__logp_m_interval__, __logp_c_interval__, __logp_likelihood)
  13.5%    27.5%       0.000s       3.43e-07s   1000     3   InplaceDimShuffle{x}(m)
  11.5%    39.0%       0.000s       2.93e-07s   1000     4   Elemwise{Composite{(i0 + (-sqr((i1 - ((i2 * i3) + i4)))))}}(TensorConstant{(1,) of -1..0664093453}, TensorConstant{[13.059668...46571405]}, InplaceDimShuffle{x}.0, TensorConstant{[0. 1. 2. .. 7. 8. 9.]}, InplaceDimShuffle{x}.0)
  11.3%    50.4%       0.000s       2.88e-07s   1000     1   Elemwise{Composite{(i0 + (i1 * scalar_sigmoid(i2)))}}(TensorConstant{-10.0}, TensorConstant{20.0}, m_interval__)
  10.8%    61.2%       0.000s       2.75e-07s   1000     5   Elemwise{Composite{((i0 + Switch(Cast{int8}((GE((i1 + i2), i1) * LE((i1 + i2), i3))), i4, i5)) - ((i6 * scalar_softplus((-i7))) + i7))}}[(0, 2)](TensorConstant{2.995732273553991}, TensorConstant{-10.0}, Elemwise{Composite{(i0 * scalar_sigmoid(i1))}}.0, TensorConstant{10.0}, TensorConstant{-2.995732273553991}, TensorConstant{-inf}, TensorConstant{2.0}, c_interval__)
   9.7%    70.9%       0.000s       2.47e-07s   1000     6   Elemwise{Composite{((i0 + Switch(Cast{int8}((GE(i1, i2) * LE(i1, i3))), i4, i5)) - ((i6 * scalar_softplus((-i7))) + i7))}}[(0, 1)](TensorConstant{2.995732273553991}, m, TensorConstant{-10.0}, TensorConstant{10.0}, TensorConstant{-2.995732273553991}, TensorConstant{-inf}, TensorConstant{2.0}, m_interval__)
   9.4%    80.3%       0.000s       2.40e-07s   1000     2   InplaceDimShuffle{x}(Elemwise{Composite{(i0 * scalar_sigmoid(i1))}}.0)
   6.0%    86.3%       0.000s       1.52e-07s   1000     0   Elemwise{Composite{(i0 * scalar_sigmoid(i1))}}(TensorConstant{20.0}, c_interval__)
   5.2%    91.4%       0.000s       1.31e-07s   1000    10   Sum{acc_dtype=float64}(MakeVector{dtype=&#39;float64&#39;}.0)
   4.5%    96.0%       0.000s       1.15e-07s   1000     7   Sum{acc_dtype=float64}(Elemwise{Composite{(i0 + (-sqr((i1 - ((i2 * i3) + i4)))))}}.0)
   4.0%   100.0%       0.000s       1.03e-07s   1000     8   Elemwise{Mul}[(0, 1)](TensorConstant{0.5}, Sum{acc_dtype=float64}.0)
   ... (remaining 0 Apply instances account for 0.00%(0.00s) of the runtime)

Here are tips to potentially make your code run faster
                 (if you think of new ones, suggest them on the mailing list).
                 Test them first, as they are not guaranteed to always provide a speedup.
  - Try the Theano flag floatX=float32
  - Try installing amdlibm and set the Theano flag lib.amdlibm=True. This speeds up only some Elemwise operation.
</pre></div></div>
</div>
<div class="section" id="Authors">
<h2>Authors<a class="headerlink" href="#Authors" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>Adapted from a blog post by <a class="reference external" href="http://mattpitkin.github.io/samplers-demo/pages/pymc3-blackbox-likelihood/">Matt
Pitkin</a>
on August 27, 2018. That post was based on an example provided by
<a class="reference external" href="https://github.com/jorgenem/">Jørgen Midtbø</a>.</li>
</ul>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../index.html">
              <img class="logo" src="../_static/pymc3_logo.jpg" alt="Logo"/>
            </a></p>
<h1 class="logo"><a href="../index.html">PyMC3</a></h1>



<p class="blurb">Probabilistic Programming in Python: Bayesian Modeling and Probabilistic Machine Learning with Theano</p>






<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../intro.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../prob_dists.html">Probability Distributions</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../examples.html">Examples</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../examples.html#howto">Howto</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="sampler-stats.html">Sampler statistics</a></li>
<li class="toctree-l3"><a class="reference internal" href="Diagnosing_biased_Inference_with_Divergences.html">Diagnosing Biased Inference with Divergences</a></li>
<li class="toctree-l3"><a class="reference internal" href="posterior_predictive.html">Posterior Predictive Checks</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_comparison.html">Model comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_averaging.html">Model averaging</a></li>
<li class="toctree-l3"><a class="reference internal" href="Bayes_factor.html">Bayes Factors and Marginal Likelihood</a></li>
<li class="toctree-l3"><a class="reference internal" href="howto_debugging.html">How to debug a model</a></li>
<li class="toctree-l3"><a class="reference internal" href="PyMC3_tips_and_heuristic.html">PyMC3 Modeling tips and heuristic</a></li>
<li class="toctree-l3"><a class="reference internal" href="LKJ.html">LKJ Cholesky Covariance Priors for Multivariate Normal Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="live_sample_plots.html">Live sample plots</a></li>
<li class="toctree-l3"><a class="reference internal" href="sampling_compound_step.html">Compound Steps in Sampling</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Using a “black box” likelihood function</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples.html#applied">Applied</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples.html#glm">GLM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples.html#survival-analysis">Survival Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples.html#gaussian-processes">Gaussian Processes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples.html#mixture-models">Mixture Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples.html#variational-inference">Variational Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples.html#stochastic-gradient">Stochastic Gradient</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../api.html">API Reference</a></li>
</ul>


<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2017, The PyMC Development Team.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.8.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../_sources/notebooks/blackbox_external_likelihood.ipynb.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>